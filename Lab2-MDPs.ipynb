{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Markov Decision Processes - Dynamic Programming\n",
    "\n",
    "## Lab Instructions\n",
    "To complete the homework, you only need to modify the code in this notebook. \n",
    "\n",
    "**You should execute every block of code to not miss any dependency.**\n",
    "\n",
    "*This exercise is adapted from Berkeley Deep RL Class [HW2](https://github.com/berkeleydeeprlcourse/homework/blob/c1027d83cd542e67ebed982d44666e0d22a00141/hw2/HW2.ipynb) [(license)](https://github.com/berkeleydeeprlcourse/homework/blob/master/LICENSE)*\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, we will solve **Markov Decision Processes (MDPs) with finite state and action spaces** with several classic algorithms that you learnt in the class.\n",
    "\n",
    "The experiments here will use the Frozen Lake environment, a simple gridworld MDP that is taken from `gym` and slightly modified for this assignment. In this MDP, the agent must navigate from the start state to the goal state on a 4x4 grid, with stochastic transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from misc import FrozenLakeEnv, make_grader\n",
    "env = FrozenLakeEnv()\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what a random episode looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Some basic imports and setup\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "# Generate the episode\n",
    "env.reset()\n",
    "for t in range(100):\n",
    "    env.render()\n",
    "    a = env.action_space.sample()\n",
    "    ob, rew, done, _ = env.step(a)\n",
    "    if done:\n",
    "        break\n",
    "assert done\n",
    "env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the episode above, the agent falls into a hole after two timesteps. Also note the stochasticity--on the first step, the DOWN action is selected, but the agent moves to the right.\n",
    "\n",
    "We extract the relevant information from the gym Env into the MDP class below.\n",
    "The ```env``` object won't be used any further, we'll just use the ```mdp``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.P is a two-level dict where the first key is the state and the second key is the action.\n",
      "The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "Action indices [0, 1, 2, 3] correspond to West, South, East and North.\n",
      "mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\n",
      "\n",
      "For example, state 0 is the initial state, and the transition information for s=0, a=0 is \n",
      "P[0][0] = [(0.1, 0, 0.0), (0.8, 0, 0.0), (0.1, 4, 0.0)] \n",
      "\n",
      "As another example, state 5 corresponds to a hole in the ice, in which all actions lead to the same state with probability 1 and reward 0.\n",
      "P[5][0] = [(1.0, 5, 0)]\n",
      "P[5][1] = [(1.0, 5, 0)]\n",
      "P[5][2] = [(1.0, 5, 0)]\n",
      "P[5][3] = [(1.0, 5, 0)]\n"
     ]
    }
   ],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, P, nS, nA, desc=None):\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "mdp = MDP( {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc)\n",
    "\n",
    "\n",
    "print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "print(np.arange(16).reshape(4,4))\n",
    "print(\"Action indices [0, 1, 2, 3] correspond to West, South, East and North.\")\n",
    "print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "print(\"As another example, state 5 corresponds to a hole in the ice, in which all actions lead to the same state with probability 1 and reward 0.\")\n",
    "for i in range(4):\n",
    "    print(\"P[5][%i] =\" % i, mdp.P[5][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: implement value iteration\n",
    "In this problem, you'll implement value iteration, which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $V^{(0)}(s)=0$, for all $s$\n",
    "\n",
    "For $i=0, 1, 2, \\dots$\n",
    "- $V^{(i+1)}(s) = \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$, for all $s$\n",
    "\n",
    "---\n",
    "\n",
    "We additionally define the sequence of greedy policies $\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}$, where\n",
    "$$\\pi^{(i)}(s) = \\arg \\max_a \\sum_{s'} P(s,a,s') [ R(s,a,s') + \\gamma V^{(i)}(s')]$$\n",
    "\n",
    "Your code will return two lists: $[V^{(0)}, V^{(1)}, \\dots, V^{(n)}]$ and $[\\pi^{(0)}, \\pi^{(1)}, \\dots, \\pi^{(n-1)}]$\n",
    "\n",
    "To ensure that you get the same policies as the reference solution, **choose the lower-index action to break ties in $\\arg \\max_a$. This is done automatically by np.argmax. This will only affect the \"# chg actions\" printout below--it won't affect the values computed.**\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Warning: make a copy of your value function each iteration and use that copy for the update--don't update your value function in place. \n",
    "Updating in-place is also a valid algorithm, sometimes called Gauss-Seidel value iteration or asynchronous value iteration, but it will cause you to get different results than our reference solution (which in turn will mean that our testing code won’t be able to help in verifying your code).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vs_VI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-aa639f6838c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualize the estimated values and corresponding policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVs_VI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpis_VI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vs_VI' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize the estimated values and corresponding policy\n",
    "for (V, pi) in zip(Vs_VI[:10], pis_VI[:10]):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(V.reshape(4,4), cmap='gray', interpolation='none', clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(4)-.5)\n",
    "    ax.set_yticks(np.arange(4)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:4, 0:4]\n",
    "    a2uv = {0: (-1, 0), 1:(0, -1), 2:(1,0), 3:(-1, 0)}\n",
    "    Pi = pi.reshape(4,4)\n",
    "    for y in range(4):\n",
    "        for x in range(4):\n",
    "            a = Pi[y, x]\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "            plt.text(x, y, str(env.desc[y,x].item().decode()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "plt.figure()\n",
    "plt.plot(Vs_VI)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Values of different states\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | max|V-Vprev| | # chg actions | V[0]\n",
      "----------+--------------+---------------+---------\n",
      "   0      | 0.80000      |  N/A          | 0.000\n",
      "   1      | 0.60800      |    2          | 0.000\n",
      "   2      | 0.51984      |    2          | 0.000\n",
      "   3      | 0.39508      |    2          | 0.000\n",
      "   4      | 0.30026      |    2          | 0.000\n",
      "   5      | 0.25355      |    1          | 0.254\n",
      "   6      | 0.10478      |    0          | 0.345\n",
      "   7      | 0.09657      |    0          | 0.442\n",
      "   8      | 0.03656      |    0          | 0.478\n",
      "   9      | 0.02772      |    0          | 0.506\n",
      "  10      | 0.01111      |    0          | 0.517\n",
      "  11      | 0.00735      |    0          | 0.524\n",
      "  12      | 0.00310      |    0          | 0.527\n",
      "  13      | 0.00190      |    0          | 0.529\n",
      "  14      | 0.00083      |    0          | 0.530\n",
      "  15      | 0.00049      |    0          | 0.531\n",
      "  16      | 0.00022      |    0          | 0.531\n",
      "  17      | 0.00013      |    0          | 0.531\n",
      "  18      | 0.00006      |    0          | 0.531\n",
      "  19      | 0.00003      |    0          | 0.531\n",
      "Test succeeded\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(mdp, gamma, nIt, grade_print=print):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mdp: MDP\n",
    "        gamma: discount factor\n",
    "        nIt: number of iterations, corresponding to n above\n",
    "    Outputs:\n",
    "        (value_functions, policies)\n",
    "        \n",
    "    len(value_functions) == nIt+1 and len(policies) == nIt\n",
    "    \"\"\"\n",
    "    grade_print(\"Iteration | max|V-Vprev| | # chg actions | V[0]\")\n",
    "    grade_print(\"----------+--------------+---------------+---------\")\n",
    "    Vs = [np.zeros(mdp.nS)] # list of value functions contains the initial value function V^{(0)}, which is zero\n",
    "    pis = []\n",
    "    for it in range(nIt):\n",
    "        oldpi = pis[-1] if len(pis) > 0 else None # \\pi^{(it)} = Greedy[V^{(it-1)}]. Just used for printout\n",
    "        Vprev = Vs[-1] # V^{(it)}\n",
    "        \n",
    "        # Your code should fill in meaningful values for the following two variables\n",
    "        # pi: greedy policy for Vprev (not V), \n",
    "        #     corresponding to the math above: \\pi^{(it)} = Greedy[V^{(it)}]\n",
    "        #     ** it needs to be numpy array of ints **\n",
    "        # V: bellman backup on Vprev\n",
    "        #     corresponding to the math above: V^{(it+1)} = T[V^{(it)}]\n",
    "        #     ** numpy array of floats **\n",
    "        \n",
    "        # >>>>> Your code\n",
    "        # hint: using 2 or 3 loop might be easy\n",
    "        V = np.array(Vprev) # REPLACE THIS LINE WITH YOUR CODE\n",
    "        pi = np.zeros(mdp.nS) # REPLACE THIS LINE WITH YOUR CODE\n",
    "        \n",
    "        states = mdp.nS\n",
    "        actions = mdp.nA\n",
    "        \n",
    "        for i in range(states):\n",
    "            value = np.zeros(actions)\n",
    "            for j in range(actions):\n",
    "                for step in mdp.P[i][j]:\n",
    "                    prob, nxt, rwd = step\n",
    "                    value[j] = value[j] + prob*(rwd + GAMMA*Vprev[nxt])\n",
    "                V[i] = np.max(value)\n",
    "                \n",
    "        for i in range(states):\n",
    "            py = np.zeros(actions)\n",
    "            for j in range(actions):\n",
    "                for step in mdp.P[i][j]:\n",
    "                    prob,nxt,rwd = step\n",
    "                    py[j] = py[j] + prob*(rwd + GAMMA*Vprev[nxt])\n",
    "                        \n",
    "            pi[i] = np.argmax(py)\n",
    "            \n",
    "                \n",
    "        \n",
    "        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        \n",
    "        \n",
    "        max_diff = np.abs(V - Vprev).max()\n",
    "        nChgActions=\"N/A\" if oldpi is None else (pi != oldpi).sum()\n",
    "        grade_print(\"%4i      | %6.5f      | %4s          | %5.3f\"%(it, max_diff, nChgActions, V[0]))\n",
    "        Vs.append(V)\n",
    "        pis.append(pi)\n",
    "    return Vs, pis\n",
    "\n",
    "GAMMA = 0.95 # we'll be using this same value in subsequent problems\n",
    "\n",
    "\n",
    "# The following is the output of a correct implementation; when\n",
    "#   this code block is run, your implementation's print output will be\n",
    "#   compared with expected output.\n",
    "#   (incorrect line in red background with correct line printed side by side to help you debug)\n",
    "expected_output = \"\"\"Iteration | max|V-Vprev| | # chg actions | V[0]\n",
    "----------+--------------+---------------+---------\n",
    "   0      | 0.80000      |  N/A          | 0.000\n",
    "   1      | 0.60800      |    2          | 0.000\n",
    "   2      | 0.51984      |    2          | 0.000\n",
    "   3      | 0.39508      |    2          | 0.000\n",
    "   4      | 0.30026      |    2          | 0.000\n",
    "   5      | 0.25355      |    1          | 0.254\n",
    "   6      | 0.10478      |    0          | 0.345\n",
    "   7      | 0.09657      |    0          | 0.442\n",
    "   8      | 0.03656      |    0          | 0.478\n",
    "   9      | 0.02772      |    0          | 0.506\n",
    "  10      | 0.01111      |    0          | 0.517\n",
    "  11      | 0.00735      |    0          | 0.524\n",
    "  12      | 0.00310      |    0          | 0.527\n",
    "  13      | 0.00190      |    0          | 0.529\n",
    "  14      | 0.00083      |    0          | 0.530\n",
    "  15      | 0.00049      |    0          | 0.531\n",
    "  16      | 0.00022      |    0          | 0.531\n",
    "  17      | 0.00013      |    0          | 0.531\n",
    "  18      | 0.00006      |    0          | 0.531\n",
    "  19      | 0.00003      |    0          | 0.531\"\"\"\n",
    "Vs_VI, pis_VI = value_iteration(mdp, gamma=GAMMA, nIt=20, grade_print=make_grader(expected_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Policy Iteration\n",
    "\n",
    "The next task is to implement exact policy iteration (PI), which has the following pseudocode:\n",
    "\n",
    "---\n",
    "Initialize $\\pi_0$\n",
    "\n",
    "For $n=0, 1, 2, \\dots$\n",
    "- Compute the state-value function $V^{\\pi_{n}}$\n",
    "- Using $V^{\\pi_{n}}$, compute the state-action-value function $Q^{\\pi_{n}}$\n",
    "- Compute new policy $\\pi_{n+1}(s) = \\operatorname*{argmax}_a Q^{\\pi_{n}}(s,a)$\n",
    "---\n",
    "\n",
    "Below, you'll implement the first and second steps of the loop step by step.\n",
    "\n",
    "### Problem 2a: state value function\n",
    "\n",
    "You'll write a function called `compute_vpi` that computes the state-value function $V^{\\pi}$ for an arbitrary policy $\\pi$.\n",
    "Recall that $V^{\\pi}$ satisfies the following linear equation:\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s,\\pi(s),s')[ R(s,\\pi(s),s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n",
    "**Here, we're going to solve the exact value function. You can solve a linear system in your code. (Find an exact solution, e.g., with `np.linalg.solve`.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# solve the exact values with `np.linalg.solve`\n",
    "def compute_vpi(pi, mdp, gamma):\n",
    "    # use pi[state] to access the action that's prescribed by this policy\n",
    "    # V[s] = P*(R + \\gamma*V[s'])\n",
    "    # => (I-\\gamma*P)*V = P*R\n",
    "    # solve linear matrix\n",
    "    # (I-\\gamma*P): (nS, nS) => a\n",
    "    # P*R => b\n",
    "    \n",
    "    # >>>>> Your code\n",
    "    a = np.zeros((mdp.nS, mdp.nS)) \n",
    "    b = np.zeros(mdp.nS)\n",
    "    states = mdp.nS\n",
    "    \n",
    "    for i in range(states) :\n",
    "        a[i][i] = 1\n",
    "        for x in mdp.P[i][pi[i]]:\n",
    "            prob, nxt, rwd = x\n",
    "            b[i] = b[i] + prob*rwd\n",
    "            a[i][nxt] = a[i][nxt] - gamma*prob\n",
    "            \n",
    "    V = np.linalg.solve(a,b)\n",
    "    \n",
    "        \n",
    "    \n",
    "    #V = np.zeros(mdp.nS) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the value of an arbitrarily-chosen policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "expected_val = np.load('compute_vpi_result.npy')\n",
    "policy = np.array([1, 0, 3, 3, 1, 3, 3, 1, 2, 2, 1, 1, 0, 3, 3, 3])\n",
    "actual_val = compute_vpi(policy, mdp, gamma=GAMMA)\n",
    "if np.all(np.isclose(actual_val, expected_val, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_val)\n",
    "    print(\"Actual: \", actual_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b: state-action value function\n",
    "\n",
    "Next, you'll write a function to compute the state-action value function $Q^{\\pi}$, defined as follows\n",
    "\n",
    "$$Q^{\\pi}(s, a) = \\sum_{s'} P(s,a,s')[ R(s,a,s') + \\gamma V^{\\pi}(s')]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "def compute_qpi(vpi, mdp, gamma):\n",
    "    # >>>>> Your code\n",
    "    Qpi = np.zeros([mdp.nS, mdp.nA]) # REPLACE THIS LINE WITH YOUR CODE\n",
    "    for i in range(mdp.nS):\n",
    "        for j in range(mdp.nA):\n",
    "            for k in mdp.P[i][j]:\n",
    "                prob, nxt, rwd = k\n",
    "                Qpi[i][j] = Qpi[i][j] + prob*(rwd + gamma*vpi[nxt])\n",
    "    \n",
    "    \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    \n",
    "    return Qpi\n",
    "\n",
    "expected_Qpi = np.array([[  0.38 ,   3.135,   1.14 ,   0.095],\n",
    "       [  0.57 ,   3.99 ,   2.09 ,   0.95 ],\n",
    "       [  1.52 ,   4.94 ,   3.04 ,   1.9  ],\n",
    "       [  2.47 ,   5.795,   3.23 ,   2.755],\n",
    "       [  3.8  ,   6.935,   4.56 ,   0.855],\n",
    "       [  4.75 ,   4.75 ,   4.75 ,   4.75 ],\n",
    "       [  4.94 ,   8.74 ,   6.46 ,   2.66 ],\n",
    "       [  6.65 ,   6.65 ,   6.65 ,   6.65 ],\n",
    "       [  7.6  ,  10.735,   8.36 ,   4.655],\n",
    "       [  7.79 ,  11.59 ,   9.31 ,   5.51 ],\n",
    "       [  8.74 ,  12.54 ,  10.26 ,   6.46 ],\n",
    "       [ 10.45 ,  10.45 ,  10.45 ,  10.45 ],\n",
    "       [ 11.4  ,  11.4  ,  11.4  ,  11.4  ],\n",
    "       [ 11.21 ,  12.35 ,  12.73 ,   9.31 ],\n",
    "       [ 12.16 ,  13.4  ,  14.48 ,  10.36 ],\n",
    "       [ 14.25 ,  14.25 ,  14.25 ,  14.25 ]])\n",
    "\n",
    "Qpi = compute_qpi(np.arange(mdp.nS), mdp, gamma=GAMMA)\n",
    "if np.all(np.isclose(expected_Qpi, Qpi, atol=1e-4)):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Expected: \", expected_Qpi)\n",
    "    print(\"Actual: \", Qpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to run the poolicy iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration | # chg actions | V[0]\n",
      "----------+---------------+---------\n",
      "   0      |      1        | -0.00000\n",
      "   1      |      9        | 0.00000\n",
      "   2      |      2        | 0.39785\n",
      "   3      |      1        | 0.45546\n",
      "   4      |      0        | 0.53118\n",
      "   5      |      0        | 0.53118\n",
      "   6      |      0        | 0.53118\n",
      "   7      |      0        | 0.53118\n",
      "   8      |      0        | 0.53118\n",
      "   9      |      0        | 0.53118\n",
      "  10      |      0        | 0.53118\n",
      "  11      |      0        | 0.53118\n",
      "  12      |      0        | 0.53118\n",
      "  13      |      0        | 0.53118\n",
      "  14      |      0        | 0.53118\n",
      "  15      |      0        | 0.53118\n",
      "  16      |      0        | 0.53118\n",
      "  17      |      0        | 0.53118\n",
      "  18      |      0        | 0.53118\n",
      "  19      |      0        | 0.53118\n",
      "Test succeeded\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47db4f7c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def policy_iteration(mdp, gamma, nIt, grade_print=print):\n",
    "    Vs = []\n",
    "    pis = []\n",
    "    pi_prev = np.zeros(mdp.nS,dtype='int')\n",
    "    pis.append(pi_prev)\n",
    "    grade_print(\"Iteration | # chg actions | V[0]\")\n",
    "    grade_print(\"----------+---------------+---------\")\n",
    "    for it in range(nIt):\n",
    "        # you need to compute qpi which is the state-action values for current pi\n",
    "        #               and compute the greedily policy, pi, from qpi\n",
    "        # >>>>> Your code (sample code are 3 lines)\n",
    "        vpi = compute_vpi(pi_prev, mdp, gamma)\n",
    "        qpi = compute_qpi(vpi, mdp, gamma)\n",
    "        pi = qpi.argmax(axis = 1)\n",
    "        \n",
    "        # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        \n",
    "        grade_print(\"%4i      | %6i        | %6.5f\"%(it, (pi != pi_prev).sum(), vpi[0]))\n",
    "        Vs.append(vpi)\n",
    "        pis.append(pi)\n",
    "        pi_prev = pi\n",
    "    return Vs, pis\n",
    "\n",
    "expected_output = \"\"\"Iteration | # chg actions | V[0]\n",
    "----------+---------------+---------\n",
    "   0      |      1        | -0.00000\n",
    "   1      |      9        | 0.00000\n",
    "   2      |      2        | 0.39785\n",
    "   3      |      1        | 0.45546\n",
    "   4      |      0        | 0.53118\n",
    "   5      |      0        | 0.53118\n",
    "   6      |      0        | 0.53118\n",
    "   7      |      0        | 0.53118\n",
    "   8      |      0        | 0.53118\n",
    "   9      |      0        | 0.53118\n",
    "  10      |      0        | 0.53118\n",
    "  11      |      0        | 0.53118\n",
    "  12      |      0        | 0.53118\n",
    "  13      |      0        | 0.53118\n",
    "  14      |      0        | 0.53118\n",
    "  15      |      0        | 0.53118\n",
    "  16      |      0        | 0.53118\n",
    "  17      |      0        | 0.53118\n",
    "  18      |      0        | 0.53118\n",
    "  19      |      0        | 0.53118\"\"\"\n",
    "\n",
    "Vs_PI, pis_PI = policy_iteration(mdp, gamma=0.95, nIt=20, grade_print=make_grader(expected_output))\n",
    "plt.plot(Vs_PI);\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.title(\"Values of different states\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Sampling-based Tabular Q-Learning\n",
    "\n",
    "So far we have implemented Value Iteration and Policy Iteration, both of which require access to an MDP's dynamics model. This requirement can sometimes be restrictive - for example, if the environment is given as a blackbox physics simulator, then we won't be able to read off the whole transition model.\n",
    "\n",
    "We can however use sampling-based Q-Learning to learn from this type of environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will learn to control a Crawler robot. Let's first try some completely random actions to see how the robot moves and familiarize ourselves with Gym environment interface again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can inspect the observation space and action space of this Gym Environment\n",
      "-----------------------------------------------------------------------------\n",
      "Action space: Discrete(4)\n",
      "It's a discrete space with 4 actions to take\n",
      "Each action corresponds to increasing/decreasing the angle of one of the joints\n",
      "We can also sample from this action space: 2\n",
      "Another action sample: 2\n",
      "Another action sample: 0\n",
      "Observation space: Tuple(Discrete(9), Discrete(13)) , which means it's a 9x13 grid.\n",
      "It's the discretized version of the robot's two joint angles\n"
     ]
    }
   ],
   "source": [
    "from crawler_env import CrawlingRobotEnv\n",
    "\n",
    "env = CrawlingRobotEnv()\n",
    "\n",
    "print(\"We can inspect the observation space and action space of this Gym Environment\")\n",
    "print(\"-----------------------------------------------------------------------------\")\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"It's a discrete space with %i actions to take\" % env.action_space.n)\n",
    "print(\"Each action corresponds to increasing/decreasing the angle of one of the joints\")\n",
    "print(\"We can also sample from this action space:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Another action sample:\", env.action_space.sample())\n",
    "print(\"Observation space:\", env.observation_space, \", which means it's a 9x13 grid.\")\n",
    "print(\"It's the discretized version of the robot's two joint angles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(\n",
    "    render=True, # turn render mode on to visualize random motion\n",
    ")\n",
    "\n",
    "# standard procedure for interfacing with a Gym environment\n",
    "cur_state = env.reset() # reset environment and get initial state\n",
    "ret = 0.\n",
    "done = False\n",
    "i = 0\n",
    "while not done:\n",
    "    action = env.action_space.sample() # sample an action randomly\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    cur_state = next_state\n",
    "    i += 1\n",
    "    if i == 1500:\n",
    "        break # for the purpose of this visualization, let's only run for 1500 steps\n",
    "        # also note the GUI won't close automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the random controller can sometimes make progress but it won't get very far. Let's implement Tabular Q-Learning with $\\epsilon$-greedy exploration to find a better policy piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values for state (0, 0): [ 0.  0.  0.  0.] which is a list of Q values for each action\n",
      "As such, the Q value of taking action 3 in state (1,2), i.e. Q((1,2), 3), can be accessed by q_vals[(1,2)][3]: 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# dictionary that maps from state, s, to a numpy array of Q values [Q(s, a_1), Q(s, a_2) ... Q(s, a_n)]\n",
    "#   and everything is initialized to 0.\n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "\n",
    "print(\"Q-values for state (0, 0): %s\" % q_vals[(0, 0)], \"which is a list of Q values for each action\")\n",
    "print(\"As such, the Q value of taking action 3 in state (1,2), i.e. Q((1,2), 3), can be accessed by q_vals[(1,2)][3]:\", q_vals[(1,2)][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1 passed\n",
      "Test2 passed\n"
     ]
    }
   ],
   "source": [
    "def eps_greedy(q_vals, eps, state):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        q_vals: q value tables\n",
    "        eps: epsilon\n",
    "        state: current state\n",
    "    Outputs:\n",
    "        random action with probability of eps; argmax Q(s, .) with probability of (1-eps)\n",
    "    \"\"\"\n",
    "    # you might want to use random.random() to implement random exploration\n",
    "    #   number of actions can be read off from len(q_vals[state])\n",
    "    import random\n",
    "    # >>>>> Your code\n",
    "    rand = random.uniform(0.0,1.0)\n",
    "    \n",
    "    if rand <= eps:\n",
    "        action = random.randint(0, len(q_vals[state])-1)\n",
    "    else:\n",
    "        action = np.argmax(q_vals[state])\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    return int(action)\n",
    "\n",
    "# test case 1\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][0] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.3, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 0) / trials\n",
    "tgt_freq = 0.3 / env.action_space.n + 0.7\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test1 passed\")\n",
    "else:\n",
    "    print(\"Test1: Expected to select 0 with frequency %.2f but got %.2f\" % (tgt_freq, freq))\n",
    "    \n",
    "# test case 2\n",
    "dummy_q = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "test_state = (0, 0)\n",
    "dummy_q[test_state][2] = 10.\n",
    "trials = 100000\n",
    "sampled_actions = [\n",
    "    int(eps_greedy(dummy_q, 0.5, test_state))\n",
    "    for _ in range(trials)\n",
    "]\n",
    "freq = np.sum(np.array(sampled_actions) == 2) / trials\n",
    "tgt_freq = 0.5 / env.action_space.n + 0.5\n",
    "if np.isclose(freq, tgt_freq, atol=1e-2):\n",
    "    print(\"Test2 passed\")\n",
    "else:\n",
    "    print(\"Test2: Expected to select 2 with frequency %.2f but got %.2f\" % (tgt_freq, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will implement Q learning update. After we observe a transition $s, a, s', r$,\n",
    "\n",
    "$$\\textrm{target}(s') = R(s,a,s') + \\gamma \\max_{a'} Q_{\\theta_k}(s',a')$$\n",
    "\n",
    "\n",
    "$$Q_{k+1}(s,a) \\leftarrow (1-\\alpha) Q_k(s,a) + \\alpha \\left[ \\textrm{target}(s') \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed\n"
     ]
    }
   ],
   "source": [
    "def q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        gamma: discount factor\n",
    "        alpha: learning rate\n",
    "        q_vals: q value table\n",
    "        cur_state: current state\n",
    "        action: action taken in current state\n",
    "        next_state: next state results from taking `action` in `cur_state`\n",
    "        reward: reward received from this transition\n",
    "    \n",
    "    Performs in-place update of q_vals table to implement one step of Q-learning\n",
    "    \"\"\"\n",
    "    # >>>>> Your code (sample code are 2 lines)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    q_vals[cur_state][action] = (1-alpha)*q_vals[cur_state][action]+alpha*(reward+gamma*np.max(q_vals[next_state])) \n",
    "    cur_state = next_state\n",
    "    \n",
    "# testing your q_learning_update implementation\n",
    "dummy_q = q_vals.copy()\n",
    "test_state = (0, 0)\n",
    "test_next_state = (0, 1)\n",
    "dummy_q[test_state][0] = 10.\n",
    "dummy_q[test_next_state][1] = 10.\n",
    "q_learning_update(0.9, 0.1, dummy_q, test_state, 0, test_next_state, 1.1)\n",
    "tgt = 10.01\n",
    "if np.isclose(dummy_q[test_state][0], tgt,):\n",
    "    print(\"Test passed\")\n",
    "else:\n",
    "    print(\"Q(test_state, 0) is expected to be %.2f but got %.2f\" % (tgt, dummy_q[test_state][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itr 0 # Average speed: 0.05\n",
      "Itr 50000 # Average speed: 2.03\n",
      "Itr 100000 # Average speed: 3.37\n",
      "Itr 150000 # Average speed: 3.37\n",
      "Itr 200000 # Average speed: 3.37\n",
      "Itr 250000 # Average speed: 3.37\n"
     ]
    }
   ],
   "source": [
    "# now with the main components tested, we can put everything together to create a complete q learning agent\n",
    "\n",
    "env = CrawlingRobotEnv() \n",
    "q_vals = defaultdict(lambda: np.array([0. for _ in range(env.action_space.n)]))\n",
    "gamma = 0.9\n",
    "alpha = 0.1\n",
    "eps = 0.5\n",
    "cur_state = env.reset()\n",
    "\n",
    "def greedy_eval():\n",
    "    \"\"\"evaluate greedy policy w.r.t current q_vals\"\"\"\n",
    "    test_env = CrawlingRobotEnv(horizon=np.inf)\n",
    "    prev_state = test_env.reset()\n",
    "    ret = 0.\n",
    "    done = False\n",
    "    H = 100\n",
    "    for i in range(H):\n",
    "        action = np.argmax(q_vals[prev_state])\n",
    "        state, reward, done, info = test_env.step(action)\n",
    "        ret += reward\n",
    "        prev_state = state\n",
    "    return ret / H\n",
    "\n",
    "for itr in range(300000):\n",
    "    # YOUR CODE HERE\n",
    "    # Hint: use eps_greedy & q_learning_update\n",
    "    # >>>>> Your code (sample code are 4 lines)\n",
    "    action = eps_greedy(q_vals, eps, cur_state)\n",
    "    next_state, reward, _, _ = env.step(action)\n",
    "    q_learning_update(gamma, alpha, q_vals, cur_state, action, next_state, reward)\n",
    "    cur_state = next_state\n",
    "       \n",
    "    \n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    \n",
    "    if itr % 50000 == 0: # evaluation\n",
    "        print(\"Itr %i # Average speed: %.2f\" % (itr, greedy_eval()))\n",
    "\n",
    "# at the end of learning your crawler should reach a speed of >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average speed should be around 3.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the learning is successful, we can visualize the learned robot controller. Remember we learn this just from interacting with the environment instead of peeking into the dynamics model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CrawlingRobotEnv(render=True, horizon=500)\n",
    "prev_state = env.reset()\n",
    "ret = 0.\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_vals[prev_state])\n",
    "    state, reward, done, info = env.step(action)\n",
    "    ret += reward\n",
    "    prev_state = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# you can close the visualization GUI with the following method \n",
    "env.close_gui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrat that you finish homework1!!!\n",
    "In homework1, we solved the exact state values (V) or state-action values (Q) with the transition function and reward function. However, in most cases, having transition function and reward function is a really strong assumption. \n",
    "\n",
    "So, in the next homework, we will assume that both transition function and reward function are unknown and use the agent's experiences (trial and error) to figure out how to maximize the accumulated reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
